"""Generate test stubs for GxP specifications.

This script parses specification files and generates pytest test stubs
for requirements that don't have corresponding tests.
"""

import re
import sys
from pathlib import Path
from typing import Dict, List, Optional, Set

import click

from pytest_gxp.markdown_format import Requirement, SpecType
from pytest_gxp.parser import SpecificationParser


def find_existing_tests(test_dir: Path) -> Dict[str, Set[str]]:
    """Find existing test files and their requirement mappings.

    Returns:
        Dict mapping test file paths to sets of requirement IDs they cover.
    """
    existing_tests: Dict[str, Set[str]] = {}
    requirement_pattern = re.compile(r'@pytest\.mark\.requirements\s*\(\s*\[([^\]]+)\]')

    if not test_dir.exists():
        return existing_tests

    for test_file in test_dir.glob("**/test_*.py"):
        try:
            content = test_file.read_text(encoding="utf-8")
            matches = requirement_pattern.findall(content)
            req_ids: Set[str] = set()
            for match in matches:
                # Parse requirement IDs from the match
                ids = re.findall(r'["\']([A-Z]{2}-\d+)["\']', match)
                req_ids.update(ids)
            existing_tests[str(test_file)] = req_ids
        except Exception:
            continue

    return existing_tests


def get_all_covered_requirements(existing_tests: Dict[str, Set[str]]) -> Set[str]:
    """Get all requirement IDs that have existing tests."""
    covered: Set[str] = set()
    for req_ids in existing_tests.values():
        covered.update(req_ids)
    return covered


def generate_test_function(requirement: Requirement) -> str:
    """Generate a test function stub for a requirement."""
    # Create a valid Python function name from the requirement ID
    func_name = f"test_{requirement.id.lower().replace('-', '_')}"

    # Extract steps from description if available
    steps = []
    if requirement.description:
        # Look for numbered lists
        step_pattern = re.compile(r'^\s*\d+\.\s*(.+)$', re.MULTILINE)
        matches = step_pattern.findall(requirement.description)
        steps = matches[:5]  # Limit to first 5 steps

    # Build the test function
    lines = [
        f'@pytest.mark.gxp',
        f'@pytest.mark.requirements(["{requirement.id}"])',
        f'def {func_name}():',
        f'    """Test {requirement.id}: {requirement.title}',
        f'',
        f'    {requirement.description[:200] if requirement.description else "No description available."}',
        f'',
        f'    Requirements: {requirement.id}',
        f'    """',
    ]

    # Add step comments
    if steps:
        for i, step in enumerate(steps, 1):
            lines.append(f'    # Step {i}: {step[:80]}')
        lines.append('')

    # Add placeholder assertion
    lines.extend([
        '    # TODO: Implement test for this requirement',
        '    raise NotImplementedError("Test not yet implemented")',
        '',
    ])

    return '\n'.join(lines)


def generate_test_file_content(
    requirements: List[Requirement],
    spec_type: SpecType,
) -> str:
    """Generate complete test file content for a list of requirements."""
    # Determine qualification phase
    phase_map = {
        SpecType.INSTALLATION: "IQ",
        SpecType.DESIGN: "OQ",
        SpecType.FUNCTIONAL: "OQ",
        SpecType.USER: "PQ",
    }
    phase = phase_map.get(spec_type, "OQ")

    lines = [
        f'"""Test stubs for {spec_type.value} Specification ({phase} Phase).',
        '',
        'Auto-generated by gxp_test_stubs. Implement each test to verify',
        'the corresponding requirement.',
        '"""',
        '',
        'import pytest',
        '',
        '',
        f'# {"=" * 76}',
        f'# {phase} Tests - {spec_type.value} Specification',
        f'# {"=" * 76}',
        '',
        '',
    ]

    for req in requirements:
        lines.append(generate_test_function(req))
        lines.append('')

    return '\n'.join(lines)


@click.command()
@click.option(
    '--spec-dir', '-s',
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    default='gxp_spec_files',
    help='Directory containing specification files (default: gxp_spec_files)',
)
@click.option(
    '--output-dir', '-o',
    type=click.Path(file_okay=False, path_type=Path),
    default='tests',
    help='Directory to write test stubs (default: tests)',
)
@click.option(
    '--test-dir', '-t',
    type=click.Path(file_okay=False, path_type=Path),
    default=None,
    help='Directory to scan for existing tests (default: same as output-dir)',
)
@click.option(
    '--prefix', '-p',
    type=str,
    default='test_gxp_',
    help='Prefix for generated test files (default: test_gxp_)',
)
@click.option(
    '--force', '-f',
    is_flag=True,
    default=False,
    help='Overwrite existing test stub files',
)
@click.option(
    '--dry-run', '-n',
    is_flag=True,
    default=False,
    help='Show what would be generated without writing files',
)
@click.option(
    '--verbose', '-v',
    is_flag=True,
    default=False,
    help='Enable verbose output',
)
def main(
    spec_dir: Path,
    output_dir: Path,
    test_dir: Optional[Path],
    prefix: str,
    force: bool,
    dry_run: bool,
    verbose: bool,
):
    """Generate pytest test stubs for GxP specification requirements.

    This tool parses GxP specification files (Installation, Design, Functional,
    User) and generates pytest test stubs for requirements that don't have
    existing tests.

    Each generated test includes:

    \b
    - @pytest.mark.gxp marker
    - @pytest.mark.requirements([...]) marker
    - Docstring with requirement details
    - Step comments extracted from requirement description
    - NotImplementedError placeholder

    Examples:

    \b
        # Generate stubs for all specs in default locations
        gxp_test_stubs

    \b
        # Generate stubs from custom spec directory
        gxp_test_stubs -s docs/specifications -o tests/gxp

    \b
        # Preview what would be generated
        gxp_test_stubs --dry-run --verbose
    """
    # Use output_dir as test_dir if not specified
    if test_dir is None:
        test_dir = output_dir

    # Check spec directory exists
    if not spec_dir.exists():
        click.echo(f"Error: Specification directory not found: {spec_dir}", err=True)
        sys.exit(1)

    # Parse specifications
    parser = SpecificationParser()
    specs = parser.parse_directory(spec_dir)

    if not specs:
        click.echo(f"No specification files found in {spec_dir}", err=True)
        sys.exit(1)

    if verbose:
        click.echo(f"Found {len(specs)} specification file(s)")

    # Find existing tests
    existing_tests = find_existing_tests(test_dir)
    covered_reqs = get_all_covered_requirements(existing_tests)

    if verbose:
        click.echo(f"Found {len(covered_reqs)} requirement(s) with existing tests")

    # Process each specification
    files_written = 0
    stubs_generated = 0

    # Order: IQ, OQ (Design, Functional), PQ
    spec_order = [
        SpecType.INSTALLATION,
        SpecType.DESIGN,
        SpecType.FUNCTIONAL,
        SpecType.USER,
    ]

    for spec_type in spec_order:
        spec = specs.get(spec_type)
        if not spec:
            continue

        # Filter to requirements without tests
        missing_reqs = [r for r in spec.requirements if r.id not in covered_reqs]

        if not missing_reqs:
            if verbose:
                click.echo(f"  {spec_type.value}: All requirements have tests")
            continue

        # Generate output filename
        type_suffix = spec_type.value.lower().replace(" ", "_")
        output_file = output_dir / f"{prefix}{type_suffix}.py"

        # Check if file exists
        if output_file.exists() and not force:
            click.echo(
                f"  {spec_type.value}: Skipping {output_file} (exists, use --force to overwrite)"
            )
            continue

        # Generate content
        content = generate_test_file_content(missing_reqs, spec_type)

        if dry_run:
            click.echo(f"\n{'=' * 60}")
            click.echo(f"Would write: {output_file}")
            click.echo(f"Requirements: {', '.join(r.id for r in missing_reqs)}")
            click.echo(f"{'=' * 60}")
            if verbose:
                click.echo(content)
        else:
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file.write_text(content, encoding="utf-8")
            click.echo(f"  Created: {output_file} ({len(missing_reqs)} stub(s))")
            files_written += 1

        stubs_generated += len(missing_reqs)

    # Summary
    click.echo()
    if dry_run:
        click.echo(f"Dry run complete: {stubs_generated} stub(s) would be generated")
    else:
        click.echo(f"Generated {stubs_generated} test stub(s) in {files_written} file(s)")

    if covered_reqs:
        click.echo(f"Skipped {len(covered_reqs)} requirement(s) with existing tests")


if __name__ == '__main__':
    main()
